# ü§ñ Git Commit AI - Three-Agent Commit Message Generator

An intelligent, fully automated pipeline that generates high-quality conventional Git commit messages from code diffs using a multi-agent system powered by **OpenChat 7B** running locally via **Ollama**.

## üìã Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Pipeline Flow](#pipeline-flow)
- [Customization](#customization)
- [Testing](#testing)
- [Troubleshooting](#troubleshooting)

## üéØ Overview

This pipeline consists of three specialized agents that work together to analyze staged Git changes and produce professional commit messages:

1. **DiffAgent** - Parses `git diff --staged` and extracts structured change information
2. **SummaryAgent** - Filters and summarizes changes into concise context
3. **CommitWriterAgent** - Crafts conventional commit messages following best practices

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Staged Code   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ git diff --staged
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   DiffAgent     ‚îÇ Parses diffs ‚Üí Bullet points
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SummaryAgent   ‚îÇ Filters/groups ‚Üí Summary
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CommitWriter    ‚îÇ Formats ‚Üí Commit message
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Final Message   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### State Management

All agents share a central `PipelineState` object:

```python
{
    "staged_diff": str,           # Raw git diff output
    "bullet_points": List[str],   # Parsed changes
    "summary": str,               # Concise summary
    "commit_message": str,        # Final output
    "errors": List[str]           # Error tracking
}
```

## üöÄ Installation

### Prerequisites

- Python 3.8 or higher
- Git CLI installed
- CUDA-capable GPU (optional, for local inference)
- 8GB+ RAM (16GB+ recommended for local inference)

### Step 1: Clone and Setup

```bash
cd c:\Users\Hp\Downloads\GenAI\code
pip install -r requirements.txt
```

### Step 2: Configure Environment

```bash
copy .env.example .env
```

Edit `.env` and configure your setup (see [Configuration](#configuration) section).

### Step 3: Setup OpenChat-3.5

#### Option A: Local Inference (Recommended)

The pipeline will automatically download the model on first run:

```python
# Model will be cached at: ~/.cache/huggingface/hub/
# First run may take 5-10 minutes to download (~7GB)
```

#### Option B: API-based Inference

If running OpenChat-3.5 via API (e.g., vLLM, text-generation-inference):

```bash
# Example with vLLM
pip install vllm
vllm serve openchat/openchat-3.5-0106 --port 8000
```

Update `.env`:

```
LLM_MODE=api
API_BASE_URL=http://localhost:8000/v1
```

## ‚öôÔ∏è Configuration

Edit `.env` file:

| Variable           | Options                              | Description                  |
| ------------------ | ------------------------------------ | ---------------------------- |
| `LLM_MODE`         | `local`, `api`                       | How to run OpenChat-3.5      |
| `LOCAL_MODEL_PATH` | Model ID/path                        | HuggingFace model identifier |
| `DEVICE`           | `cuda`, `cpu`, `mps`                 | Computation device           |
| `DEBUG_MODE`       | `true`, `false`                      | Print intermediate outputs   |
| `COMMIT_STYLE`     | `conventional`, `angular`, `gitmoji` | Commit format                |

## üìñ Usage

### Basic Usage

```bash
# Stage your changes
git add .

# Run the pipeline
python main.py

# The commit message will be displayed and optionally committed
```

### Command-Line Options

```bash
# Generate commit message only (don't commit)
python main.py --dry-run

# Verbose mode (show all agent outputs)
python main.py --verbose

# Use specific repository path
python main.py --repo-path /path/to/repo

# Auto-commit without confirmation
python main.py --auto-commit

# Output to file
python main.py --output commit_msg.txt
```

### Programmatic Usage

```python
from pipeline import CommitPipeline

# Initialize pipeline
pipeline = CommitPipeline(repo_path=".", debug=True)

# Run all agents
result = pipeline.run()

# Access outputs
print(result.commit_message)
print(result.summary)
print(result.bullet_points)
```

## üîÑ Pipeline Flow

### 1. DiffAgent

```python
Input:  git diff --staged (raw text)
Output: [
    "‚Ä¢ Added new function `calculate_total()` in utils/math.py",
    "‚Ä¢ Updated API endpoint `/users` in routes/api.py",
    "‚Ä¢ Removed deprecated `oldFunction()` from legacy/code.py"
]
```

### 2. SummaryAgent

```python
Input:  Bullet points from DiffAgent
Output: "Implemented new calculation utilities and updated user API
         endpoint. Removed deprecated legacy functions."
```

### 3. CommitWriterAgent

```python
Input:  Summary from SummaryAgent
Output: "feat(api): implement calculation utilities and update user endpoint

- Add calculate_total function for arithmetic operations
- Update /users API endpoint with new response format
- Remove deprecated functions from legacy codebase"
```

## üé® Customization

### Custom Commit Templates

Edit `agents/commit_writer_agent.py` to customize the prompt:

```python
COMMIT_TEMPLATE = """
Your custom instructions here...
"""
```

### Adding New Agents

```python
from agents.base_agent import BaseAgent

class MyCustomAgent(BaseAgent):
    def process(self, state: PipelineState) -> PipelineState:
        # Your logic here
        return state
```

### Using Different LLMs

Modify `llm_client.py` to support other models:

```python
class LLMClient:
    def __init__(self, model_name: str = "your-model"):
        # Custom initialization
        pass
```

## üß™ Testing

Run the test suite:

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=. --cov-report=html

# Run specific test
pytest tests/test_diff_agent.py -v
```

### Example Test

```python
python tests/example_usage.py
```

This will:

1. Create a test repository
2. Make sample changes
3. Run the full pipeline
4. Display outputs from each agent

## üîß Troubleshooting

### Model Download Issues

```bash
# Pre-download the model
python -c "from transformers import AutoModelForCausalLM; AutoModelForCausalLM.from_pretrained('openchat/openchat-3.5-0106')"
```

### Memory Issues (Local Inference)

```python
# In .env, use 8-bit quantization
USE_8BIT=true
```

### Git Repository Not Found

```bash
# Ensure you're in a git repository
git init
git add .
```

### API Connection Errors

```bash
# Test your API endpoint
curl http://localhost:8000/v1/models
```

## üìö Resources

- [OpenChat-3.5 Documentation](https://huggingface.co/openchat/openchat-3.5-0106)
- [Conventional Commits Spec](https://www.conventionalcommits.org/)
- [GitPython Documentation](https://gitpython.readthedocs.io/)

## üìÑ License

MIT License - feel free to use and modify as needed.

## ü§ù Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

**Built with ‚ù§Ô∏è using OpenChat-3.5 and Python**
