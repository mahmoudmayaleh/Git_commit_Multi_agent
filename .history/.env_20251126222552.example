# OpenChat-3.5 Configuration
# Copy this file to .env and fill in your values

# ===== LLM Configuration =====
# Options: "local" or "api"
LLM_MODE=local

# For local inference
LOCAL_MODEL_PATH=openchat/openchat-3.5-0106
DEVICE=cuda  # Options: cuda, cpu, mps (for Mac M1/M2)
MAX_NEW_TOKENS=512
TEMPERATURE=0.7

# For API-based inference (if LLM_MODE=api)
API_BASE_URL=http://localhost:8000/v1
API_KEY=your-api-key-here
API_MODEL=openchat-3.5

# ===== Git Configuration =====
# Repository path (leave empty to use current directory)
GIT_REPO_PATH=

# ===== Pipeline Configuration =====
# Enable debug mode to print intermediate outputs
DEBUG_MODE=true

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Commit message style (conventional, angular, gitmoji)
COMMIT_STYLE=conventional
