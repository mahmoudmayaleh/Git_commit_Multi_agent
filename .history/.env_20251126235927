# OpenChat-3.5 Configuration
# Copy this file to .env and fill in your values

# ===== LLM Configuration =====
# Options: "local" or "api"
# Using API mode with Ollama (local OpenChat-3.5 server)
LLM_MODE=api

# For local inference with OpenChat-3.5 (open source)
LOCAL_MODEL_PATH=openchat/openchat-3.5-0106
DEVICE=cpu  # Options: cuda, cpu, mps (for Mac M1/M2)
MAX_NEW_TOKENS=512
TEMPERATURE=0.7
USE_8BIT=false

# For API-based inference (if LLM_MODE=api)
# Using Ollama local server (free, open-source, optimized)
API_BASE_URL=http://localhost:11434/v1
API_KEY=ollama
API_MODEL=openchat:7b

# ===== Git Configuration =====
# Repository path (leave empty to use current directory)
GIT_REPO_PATH=

# ===== Pipeline Configuration =====
# Enable debug mode to print intermediate outputs
DEBUG_MODE=true

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Commit message style (conventional, angular, gitmoji)
COMMIT_STYLE=conventional
