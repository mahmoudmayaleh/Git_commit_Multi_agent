# OpenChat-3.5 Configuration
# Copy this file to .env and fill in your values

# ===== LLM Configuration =====
# Options: "local" or "api"
# Using LOCAL mode with open-source OpenChat-3.5 model
LLM_MODE=local

# For local inference with OpenChat-3.5 (open source)
LOCAL_MODEL_PATH=openchat/openchat-3.5-0106
DEVICE=cpu  # Options: cuda, cpu, mps (for Mac M1/M2)
MAX_NEW_TOKENS=512
TEMPERATURE=0.7
USE_8BIT=false

# For API-based inference (if LLM_MODE=api)
# You can use OpenAI API, local vLLM server, or any OpenAI-compatible endpoint
API_BASE_URL=https://api.openai.com/v1
API_KEY=your-api-key-here
API_MODEL=gpt-3.5-turbo

# ===== Git Configuration =====
# Repository path (leave empty to use current directory)
GIT_REPO_PATH=

# ===== Pipeline Configuration =====
# Enable debug mode to print intermediate outputs
DEBUG_MODE=true

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Commit message style (conventional, angular, gitmoji)
COMMIT_STYLE=conventional
