# Git Commit AI Configuration
# Copy this file to .env and customize as needed
#
# This configuration uses Ollama (https://ollama.com) for local LLM inference
# Make sure you have:
# 1. Installed Ollama: https://ollama.com/download
# 2. Pulled a model: ollama pull openchat:7b

# ===== LLM Configuration =====
# Mode: "api" for Ollama (recommended) or "local" for HuggingFace transformers
LLM_MODE=api

# ===== Ollama Settings (when LLM_MODE=api) =====
# Ollama API endpoint (default local installation)
API_BASE_URL=http://localhost:11434/v1

# Any value works for API_KEY (Ollama doesn't require authentication)
API_KEY=ollama

# Model name in Ollama (run 'ollama list' to see installed models)
# Popular options:
#   openchat:7b      - Best for commit messages (recommended)
#   llama2:7b        - Good alternative
#   codellama:7b     - Optimized for code
#   mistral:7b       - Fast and capable
API_MODEL=openchat:7b

# ===== Local Inference Settings (when LLM_MODE=local) =====
# Only used if you prefer HuggingFace transformers over Ollama
# Not recommended - Ollama is faster and easier to use
LOCAL_MODEL_PATH=openchat/openchat-3.5-0106
DEVICE=cpu  # Options: cuda, cpu, mps (for Mac M1/M2)
USE_8BIT=false

# ===== Generation Parameters =====
# Maximum tokens to generate (higher = longer messages)
MAX_NEW_TOKENS=512

# Temperature (0.0-1.0, higher = more creative)
TEMPERATURE=0.7

# ===== Git Configuration =====
# Repository path (leave empty to use current directory)
GIT_REPO_PATH=

# ===== Pipeline Configuration =====
# Show intermediate agent outputs (recommended for first use)
DEBUG_MODE=true

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Commit message style
# Options: conventional (recommended), angular, gitmoji
COMMIT_STYLE=conventional

# ===== Quick Start =====
# 1. Install Ollama: https://ollama.com/download
# 2. Pull model: ollama pull openchat:7b
# 3. Copy this file: cp .env.example .env
# 4. Test: python main.py --dry-run

